{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1v975f1J9M-WUGCd_oujTeSnBm3qRnpPF","timestamp":1736363508804}],"gpuType":"V28","machine_shape":"hm","authorship_tag":"ABX9TyOV9sPjvyVsF0o0muk2kinG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install datasets peft trl torch colorama tabulate pycryptodome pymupdf python-docx tqdm mpi4py\n","!pip install transformers --upgrade\n","!pip install deepspeed --upgrade"],"metadata":{"id":"4EUV4hZC2qrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install colorama"],"metadata":{"id":"sKaMlZlzTHSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y tensorflow"],"metadata":{"id":"cLxl25lOU05b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow-cpu"],"metadata":{"id":"UMQtnfFHU7ue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"svXRL-P6U_Er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/python3\n","\n","# Loads Google Drive\n","\n","import os\n","import shutil\n","from google.colab import drive\n","from colorama import Fore, Style, init\n","from IPython.display import display, HTML\n","\n","# Initialize colorama\n","init()\n","\n","# Access the API key from Colab Secrets\n","from google.colab import userdata\n","\n","try:\n","    api_key = userdata.get('GOOGLE_DRIVE_API')\n","    print(f\"{Fore.GREEN}‚úÖ API Key loaded successfully.{Style.RESET_ALL}\")\n","\n","    # Set the API key as an environment variable\n","    os.environ['GOOGLE_DRIVE_API'] = api_key\n","except Exception as e:\n","    print(f\"{Fore.RED}‚ùå Error loading API Key: {e}{Style.RESET_ALL}\")\n","    api_key = None\n","\n","# Mount Google Drive using the API key (if available)\n","if api_key:\n","    try:\n","        # Use the API key to authenticate and mount Google Drive\n","        drive.mount('/content/drive', force_remount=True)\n","        print(f\"{Fore.GREEN}‚úÖ Google Drive mounted successfully using API Key.{Style.RESET_ALL}\")\n","    except Exception as e:\n","        print(f\"{Fore.RED}‚ùå Error mounting Google Drive: {e}{Style.RESET_ALL}\")\n","else:\n","    print(f\"{Fore.YELLOW}‚ö†Ô∏è No API Key found. Please log in manually.{Style.RESET_ALL}\")\n","    drive.mount('/content/drive', force_remount=True)\n","\n","# Define the Google Drive folder path\n","drive_folder = \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning\"\n","\n","# Define the Colab working directory\n","colab_folder = \"/content/\"\n","\n","# Ensure the Colab folder exists\n","os.makedirs(colab_folder, exist_ok=True)\n","\n","# Function to count files in a directory\n","def count_files(directory):\n","    return sum([len(files) for _, _, files in os.walk(directory)])\n","\n","# Function to sync files from Colab to Google Drive\n","def sync_to_drive():\n","    try:\n","        print(f\"{Fore.CYAN}üîÑ Syncing files from Colab to Google Drive...{Style.RESET_ALL}\")\n","\n","        # Use rsync to copy files from Colab to Google Drive\n","        os.system(f\"rsync -av --progress {colab_folder}/ {drive_folder}/\")\n","\n","        print(f\"{Fore.GREEN}‚úÖ Files synced from Colab to Google Drive.{Style.RESET_ALL}\")\n","    except Exception as e:\n","        print(f\"{Fore.RED}‚ùå Error syncing to Google Drive: {e}{Style.RESET_ALL}\")\n","\n","# Function to sync files from Google Drive to Colab\n","def sync_from_drive():\n","    try:\n","        print(f\"{Fore.CYAN}üîÑ Syncing files from Google Drive to Colab...{Style.RESET_ALL}\")\n","\n","        # Count files before sync\n","        initial_count = count_files(colab_folder)\n","\n","        # Use rsync to copy files from Google Drive to Colab\n","        # -a: Archive mode (recursive, preserves permissions, symlinks, etc.)\n","        # -v: Verbose output\n","        # --progress: Show progress during transfer\n","        # --ignore-existing: Skip files that already exist in the destination\n","        # old command = os.system(f\"rsync -av --progress {drive_folder}/ {colab_folder}/\")\n","        os.system(f\"rsync -av --progress {drive_folder} {colab_folder}/\")\n","        # Count files after sync\n","        final_count = count_files(colab_folder)\n","        files_transferred = final_count - initial_count\n","\n","        print(f\"{Fore.GREEN}‚úÖ Sync completed!{Style.RESET_ALL}\")\n","        print(f\"{Fore.GREEN}üìÇ Files transferred: {files_transferred}{Style.RESET_ALL}\")\n","\n","        # List files in the Colab folder to confirm sync\n","        print(f\"{Fore.BLUE}üìÅ Files in Colab folder:{Style.RESET_ALL}\")\n","        for root, dirs, files in os.walk(colab_folder):\n","            for file in files:\n","                print(os.path.join(root, file))\n","    except Exception as e:\n","        print(f\"{Fore.RED}‚ùå Error syncing from Google Drive: {e}{Style.RESET_ALL}\")\n","\n","# Function to display a sync button\n","def display_sync_button():\n","    display(HTML('''\n","        <div>\n","            <button onclick=\"syncFromDrive()\">Sync from Google Drive to Colab</button>\n","            <button onclick=\"syncToDrive()\">Sync from Colab to Google Drive</button>\n","            <button onclick=\"refreshPage()\">Refresh</button>\n","        </div>\n","        <script>\n","            function syncFromDrive() {\n","                google.colab.kernel.invokeFunction('sync_from_drive', [], {});\n","            }\n","            function syncToDrive() {\n","                google.colab.kernel.invokeFunction('sync_to_drive', [], {});\n","            }\n","            function refreshPage() {\n","                window.location.reload();\n","            }\n","        </script>\n","    '''))\n","\n","# Register the sync functions with Colab\n","from google.colab import output\n","output.register_callback('sync_from_drive', sync_from_drive)\n","output.register_callback('sync_to_drive', sync_to_drive)\n","\n","\n","# Display the sync button\n","display_sync_button()"],"metadata":{"id":"Rk934UEi3Epy"},"execution_count":null,"outputs":[]},{"source":["import sqlite3\n","\n","# Your database code here\n","conn = sqlite3.connect('/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/db/data.db')\n","cursor = conn.cursor()\n","\n","print(\"Connected to Database.\")"],"cell_type":"code","metadata":{"id":"z51jb7gn4e5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Clears any cache cuda used in last run\n","\n","torch.cuda.empty_cache()\n","\n","print(\"CUDA cache cleared.\")"],"metadata":{"id":"WujM4_5_Ksws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","import deepspeed\n","\n","print(f\"Transformers version: {transformers.__version__}\")\n","print(f\"DeepSpeed version: {deepspeed.__version__}\")"],"metadata":{"id":"bW7HVnIIesOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/python3\n","\n","# Loads Model from Google Drive\n","\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from accelerate import Accelerator\n","import os\n","import torch\n","\n","# Clear GPU memory\n","torch.cuda.empty_cache()\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the model name and save directory\n","MODEL_NAME = \"microsoft/phi-3-mini-4k-instruct\"\n","DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/models\"\n","\n","# Ensure the save directory exists\n","os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n","\n","# Function to check if the model files exist in the directory\n","def model_files_exist(directory):\n","    required_files = [\n","        \"tokenizer_config.json\",\n","        \"tokenizer.model\",\n","        \"tokenizer.json\",\n","        \"added_tokens.json\",\n","        \"special_tokens_map.json\",\n","        \"config.json\",\n","        \"model.safetensors.index.json\",\n","        \"generation_config.json\",\n","    ]\n","    # Check for safetensors files (e.g., model-00001-of-00002.safetensors)\n","    safetensors_files = [f for f in os.listdir(directory) if f.startswith(\"model-\") and f.endswith(\".safetensors\")]\n","    if not safetensors_files:\n","        return False\n","    # Check if all required files exist\n","    for file in required_files:\n","        if not os.path.exists(os.path.join(directory, file)):\n","            return False\n","    return True\n","\n","# Function to download and save the model\n","def download_and_save_model():\n","    print(f\"Downloading model: {MODEL_NAME}...\")\n","\n","    # Download the tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","\n","    # Save the tokenizer and model to Google Drive\n","    tokenizer.save_pretrained(DRIVE_MODEL_DIR)\n","    model.save_pretrained(DRIVE_MODEL_DIR)\n","\n","    print(f\"Model and tokenizer saved to Google Drive: {DRIVE_MODEL_DIR}\")\n","\n","# Check if the model is already downloaded\n","if not model_files_exist(DRIVE_MODEL_DIR):\n","    download_and_save_model()\n","else:\n","    print(f\"Model already exists in Google Drive: {DRIVE_MODEL_DIR}\")\n","\n","# Load the tokenizer and model from the specified directory\n","print(\"Loading model and tokenizer from Google Drive...\")\n","tokenizer = AutoTokenizer.from_pretrained(DRIVE_MODEL_DIR)\n","model = AutoModelForCausalLM.from_pretrained(DRIVE_MODEL_DIR)\n","\n","print(\"Model and tokenizer loaded successfully!\")"],"metadata":{"id":"dPdhMB2uwsvy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"if-kTOOyPlGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the tokenizer and model from Google Drive\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/models\")\n","model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/models\")\n","\n","print(\"Model and tokenizer loaded from Google Drive.\")"],"metadata":{"id":"3LLmVj3cyPWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf /content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/results/*"],"metadata":{"id":"jDZA8Qw-ApZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf /root/.cache/torch_extensions\n","!rm -rf /root/.cache/huggingface\n","!rm -rf /root/.cache/deepspeed"],"metadata":{"id":"SI1qC4ZVAt7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/python3\n","\n","# Loads Model from Google Drive and Starts Training on TPU.\n","\n","import os\n","import json\n","import torch\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n","from datasets import Dataset\n","from colorama import Fore, Style, init\n","import re\n","import random\n","from packaging import version\n","from IPython.display import clear_output\n","\n","# Initialize colorama\n","init()\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define paths\n","MODEL_NAME = \"microsoft/phi-3-mini-4k-instruct\"\n","DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/models\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/results\"\n","DATA_PATH = \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/db/exported_data.json\"\n","\n","# Ensure directories exist\n","os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Install torch_xla for TPU support\n","print(f\"{Fore.CYAN}Installing torch_xla for TPU support...{Style.RESET_ALL}\")\n","!pip install cloud-tpu-client torch torchvision torch_xla\n","print(f\"{Fore.GREEN}torch_xla installed successfully!{Style.RESET_ALL}\")\n","\n","# Import torch_xla\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","\n","# Set environment variable to reduce memory fragmentation\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# Function to load the model with checkpointing\n","def load_model():\n","    print(f\"{Fore.CYAN}Loading model from Google Drive...{Style.RESET_ALL}\")\n","    if os.path.exists(DRIVE_MODEL_DIR):\n","        print(f\"{Fore.GREEN}Model found in Google Drive. Loading...{Style.RESET_ALL}\")\n","        tokenizer = AutoTokenizer.from_pretrained(DRIVE_MODEL_DIR)\n","        model = AutoModelForCausalLM.from_pretrained(DRIVE_MODEL_DIR)\n","    else:\n","        print(f\"{Fore.YELLOW}Model not found in Google Drive. Downloading from Hugging Face...{Style.RESET_ALL}\")\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","        # Save the model to Google Drive for future use\n","        tokenizer.save_pretrained(DRIVE_MODEL_DIR)\n","        model.save_pretrained(DRIVE_MODEL_DIR)\n","        print(f\"{Fore.GREEN}Model saved to Google Drive at {DRIVE_MODEL_DIR}.{Style.RESET_ALL}\")\n","\n","    # Enable gradient checkpointing\n","    model.gradient_checkpointing_enable()\n","    return tokenizer, model\n","\n","# Load the model and tokenizer\n","tokenizer, model = load_model()\n","if tokenizer is None or model is None:\n","    raise Exception(\"Failed to load the model.\")\n","\n","# Function to clean the text data\n","def clean_text(text):\n","    # Remove any non-alphanumeric characters and extra whitespaces\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Load and preprocess the data\n","def load_and_preprocess_data(data_path, sample_size=None):  # Remove sample_size for full training\n","    print(f\"{Fore.CYAN}Loading and preprocessing data...{Style.RESET_ALL}\")\n","    with open(data_path, \"r\") as f:\n","        data = json.load(f)\n","\n","    training_data = []\n","    for item in data:\n","        try:\n","            content = json.loads(item[\"content\"])\n","            if \"text\" in content:\n","                cleaned_text = clean_text(content[\"text\"])\n","                if cleaned_text:\n","                    training_data.append(cleaned_text)\n","            else:\n","                cleaned_text = clean_text(item[\"content\"])\n","                if cleaned_text:\n","                    training_data.append(cleaned_text)\n","        except json.JSONDecodeError:\n","            cleaned_text = clean_text(item[\"content\"])\n","            if cleaned_text:\n","                training_data.append(cleaned_text)\n","\n","    # Shuffle the data\n","    random.shuffle(training_data)\n","\n","    # Use full dataset for training\n","    if sample_size and len(training_data) > sample_size:\n","        training_data = training_data[:sample_size]\n","\n","    print(f\"{Fore.GREEN}Data loaded and preprocessed with {len(training_data)} samples.{Style.RESET_ALL}\")\n","    return training_data\n","\n","# Load and preprocess data\n","training_data = load_and_preprocess_data(DATA_PATH)  # Use full dataset\n","\n","# Convert to a Hugging Face Dataset\n","dataset = Dataset.from_dict({\"text\": training_data})\n","\n","# Tokenize the dataset with reduced sequence length\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)  # Increase max_length\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","# Add labels for causal language modeling\n","tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=8,  # Adjust batch size for TPU\n","    num_train_epochs=3,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    logging_dir=\"./logs\",\n","    logging_steps=1,  # Log every step\n","    learning_rate=5e-5,  # Set learning rate explicitly\n","    adam_beta1=0.9,  # Set beta1 explicitly\n","    adam_beta2=0.999,  # Set beta2 explicitly\n","    report_to=\"none\",  # Disable wandb logging\n",")\n","\n","# Custom Trainer class for TPU training\n","class TPUTrainer(Trainer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.device = xm.xla_device()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        # Check if labels are present before unpacking\n","        if \"labels\" in inputs:\n","            labels = inputs.pop(\"labels\")\n","        else:\n","            labels = None  # No labels means no loss is calculated\n","\n","        outputs = model(**inputs)\n","\n","        if labels is not None:\n","            # If labels are provided, calculate the loss\n","            loss_fct = torch.nn.CrossEntropyLoss()\n","            shift_logits = outputs.logits[..., :-1, :].contiguous()\n","            shift_labels = labels[..., 1:].contiguous()\n","\n","            # Get the vocabulary size from the model's configuration\n","            vocab_size = model.config.vocab_size\n","\n","            # Debug: Print shapes and vocab size\n","            print(f\"{Fore.BLUE}Shift logits shape: {shift_logits.shape}{Style.RESET_ALL}\")\n","            print(f\"{Fore.BLUE}Shift labels shape: {shift_labels.shape}{Style.RESET_ALL}\")\n","            print(f\"{Fore.BLUE}Vocab size: {vocab_size}{Style.RESET_ALL}\")\n","\n","            # Reshape logits and labels for loss calculation\n","            shift_logits = shift_logits.view(-1, vocab_size)\n","            shift_labels = shift_labels.view(-1)\n","\n","            # Debug: Print reshaped shapes\n","            print(f\"{Fore.BLUE}Reshaped shift logits shape: {shift_logits.shape}{Style.RESET_ALL}\")\n","            print(f\"{Fore.BLUE}Reshaped shift labels shape: {shift_labels.shape}{Style.RESET_ALL}\")\n","\n","            # Calculate loss\n","            loss = loss_fct(shift_logits, shift_labels)\n","\n","            print(f\"{Fore.GREEN}Loss calculated: {loss.item()}{Style.RESET_ALL}\")  # Debug: Print Loss\n","        else:\n","            loss = outputs.loss  # If no labels provided, use original loss\n","            print(f\"{Fore.YELLOW}No labels provided. Using original loss {loss.item()}{Style.RESET_ALL}\")  # Debug: Print Loss\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Initialize the Trainer with the custom class\n","trainer = TPUTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n",")\n","\n","# Clear GPU cache before training\n","torch.cuda.empty_cache()\n","\n","# Train the model with graceful interruption handling\n","try:\n","    print(f\"{Fore.CYAN}=== Starting Training ==={Style.RESET_ALL}\")\n","    trainer.train()\n","except KeyboardInterrupt:\n","    print(f\"{Fore.YELLOW}Training interrupted by the user. Saving the model...{Style.RESET_ALL}\")\n","    trainer.save_model(f\"{OUTPUT_DIR}/interrupted_model\")\n","    print(f\"{Fore.GREEN}Model saved to '{OUTPUT_DIR}/interrupted_model'.{Style.RESET_ALL}\")"],"metadata":{"id":"QaD9Sy4Iy__y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import shutil\n","\n","# Create a zip file of the fine-tuned model\n","shutil.make_archive(\"fine_tuned_phi3\", 'zip', \"/content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/model\")\n","\n","# Download the zip file\n","files.download(\"fine_tuned_phi3.zip\")"],"metadata":{"id":"0v1vU2933AOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1G82GuO-tez"},"outputs":[],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print(f\"Your runtime has {ram_gb:.1f} gigabytes of available RAM\")\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["# prompt: A nice welcome banner\n","\n","import colorama\n","from colorama import Fore, Style\n","\n","colorama.init()\n","\n","def print_banner():\n","  banner_text = r\"\"\"\n","  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n","  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n","  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù\n","  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n","  ‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë\n","  ‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù\n","  \"\"\"\n","\n","  print(Fore.GREEN + Style.BRIGHT + banner_text + Style.RESET_ALL)\n","  print(Fore.YELLOW + \"Dependencies installed successfully!\" + Style.RESET_ALL)\n","\n","\n","print_banner()"],"metadata":{"id":"jq4C4MNnfSR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Create folders\n","os.makedirs(\"inputs\", exist_ok=True)\n","os.makedirs(\"outputs/phi3_converted\", exist_ok=True)\n","os.makedirs(\"db\", exist_ok=True)\n","os.makedirs(\"db/backups\", exist_ok=True)\n","\n","print(\"Folder structure created.\")"],"metadata":{"id":"ydU68K6G2uur"},"execution_count":null,"outputs":[]},{"source":["# Load a pre-trained model\n","model_name = \"microsoft/phi-3-mini-4k-instruct\"  # Replace with your desired model\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Or, define your custom PyTorch model\n","# class MyModel(torch.nn.Module):\n","#     # ... your model definition ...\n","# model = MyModel()"],"cell_type":"code","metadata":{"id":"jMJRiVtNQq_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y transformers\n","!pip uninstall -y deepspeed"],"metadata":{"id":"YVzAkVDFellf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/training_script.py"],"metadata":{"id":"_6210GKS29LU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/database.py"],"metadata":{"id":"JV56Qsua24AD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install Crypto docx exceptions"],"metadata":{"id":"QWJHCLTIDyMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/drive/MyDrive/Colab_Projects/Phi3_FineTuning/convert_to_phi3_unified.py"],"metadata":{"id":"IgMfl0Ke20-U"},"execution_count":null,"outputs":[]}]}